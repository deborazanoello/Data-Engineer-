{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from datetime import datetime, timedelta\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Definições do DAG\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2023, 1, 1),\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "dag = DAG('etl_postgres_to_databricks', default_args=default_args, schedule_interval='@daily')\n",
    "\n",
    "# Função para extrair dados do PostgreSQL\n",
    "def extract_data():\n",
    "    con = psycopg2.connect(host='your-host', database='your-database', user='your-user', password='your-password')\n",
    "    query = \"\"\"\n",
    "    SELECT q.id_quote AS \"Quote Id\",\n",
    "           q.nr_quote AS \"Quote Number\",\n",
    "           q.id_opportunity AS \"Opportunity Id\",\n",
    "           q.id_vendedor AS \"Seller Id\",\n",
    "           q.id_cliente AS \"Client Id\",\n",
    "           q.id_order AS \"Order Id\",\n",
    "           q.id_especificador AS \"Specifier Id\",\n",
    "           q.ds_status AS \"Quote Status\",\n",
    "           q.dt_lastmodified::date AS \"Modification Date\",\n",
    "           q.dt_emissao AS \"Issuance Date\",\n",
    "           q.dt_prev_fechamento AS \"Expected Closing Date\",\n",
    "           q.fl_orcamento_principal AS \"Main Quote\",\n",
    "           q.dt_desejo AS \"Desired Date\",\n",
    "           q.dt_expiracao AS \"Expiration Date\",\n",
    "           q.cd_motivo_cancelamento AS \"Cancellation Reason Code\",\n",
    "           q.ds_motivo_cancelamento AS \"Cancellation Reason Description\",\n",
    "           dr.ds_name AS \"Sales Type\",\n",
    "           sq.id AS \"Quote Item Id\",\n",
    "           sq.nm_ambiente AS \"Environment\",\n",
    "           sq.fl_corte_especial AS \"Special Cut\",\n",
    "           ds_classificacao AS \"Product Classification\",\n",
    "           ds_formato AS \"Product Format\",\n",
    "           ds_grandes_formatos AS \"Large Format Products\",\n",
    "           ds_linha AS \"Product Line\",\n",
    "           nm_produto AS \"Product Name\",\n",
    "           ds_tipologia_cml AS \"Commercial Typology\",\n",
    "           ds_tipologia AS \"Main Typology\",\n",
    "           cd_unimed AS \"Unit of Measure\",\n",
    "           ds_fase_vida AS \"Life Stage\",\n",
    "           class_port_1 AS \"Product Category 1\",\n",
    "           class_port_2 AS \"Product Category 2\",\n",
    "           class_port_3 AS \"Product Category 3\",\n",
    "           class_port_4 AS \"Product Category 4\"\n",
    "    FROM your_schema.stage_quote q\n",
    "    LEFT JOIN your_schema.stage_quotelineitem sq ON sq.id_quote = q.id_quote\n",
    "    LEFT JOIN your_schema.dim_product2 p ON p.id = sq.id_product2\n",
    "    LEFT JOIN your_schema.dim_produto_orc po ON po.cod_produto_ora = p.id_productcode\n",
    "    LEFT JOIN your_schema.dim_recordtype dr ON dr.id = q.recordtype_id\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(query, con)\n",
    "    df.to_csv('/tmp/raw_data.csv', index=False)\n",
    "    con.close()\n",
    "\n",
    "# Função para transformar dados\n",
    "def transform_data():\n",
    "    df = pd.read_csv('/tmp/raw_data.csv')\n",
    "    df['Year'] = pd.to_datetime(df['Issuance Date'], utc=True).dt.year\n",
    "    df.to_csv('/tmp/transformed_data.csv', index=False)\n",
    "\n",
    "# Função para carregar dados no Databricks\n",
    "def load_data():\n",
    "    df = pd.read_csv('/tmp/transformed_data.csv')\n",
    "    # Aqui você pode usar a API do Databricks para carregar os dados no Databricks\n",
    "    url = \"https://your-databricks-instance/api/2.0/dbfs/put\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {your_access_token}\"\n",
    "    }\n",
    "    files = {'file': open('/tmp/transformed_data.csv', 'rb')}\n",
    "    response = requests.post(url, headers=headers, files=files)\n",
    "    response.raise_for_status()\n",
    "\n",
    "# Tarefas do DAG\n",
    "start_task = DummyOperator(task_id='start', dag=dag)\n",
    "\n",
    "extract_task = PythonOperator(task_id='extract_data', python_callable=extract_data, dag=dag)\n",
    "\n",
    "transform_task = PythonOperator(task_id='transform_data', python_callable=transform_data, dag=dag)\n",
    "\n",
    "load_task = PythonOperator(task_id='load_data', python_callable=load_data, dag=dag)\n",
    "\n",
    "end_task = DummyOperator(task_id='end', dag=dag)\n",
    "\n",
    "# Definição da sequência das tarefas\n",
    "start_task >> extract_task >> transform_task >> load_task >> end_task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Databricks notebook source\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# Iniciar sessão Spark\n",
    "spark = SparkSession.builder.appName(\"SalesDataAnalysis\").getOrCreate()\n",
    "\n",
    "# Carregar dados de vendas\n",
    "sales_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/dbfs/tmp/transformed_data.csv\")\n",
    "\n",
    "# Exibir esquema do DataFrame\n",
    "sales_df.printSchema()\n",
    "\n",
    "# Transformar dados\n",
    "sales_df = sales_df.withColumnRenamed(\"Quote Id\", \"Quote ID\")\n",
    "sales_df = sales_df.withColumnRenamed(\"Quote Status\", \"Status\")\n",
    "sales_df = sales_df.withColumn(\"Year\", sales_df[\"Issuance Date\"].substr(0, 4))\n",
    "\n",
    "# Converter para Pandas DataFrame para análise adicional\n",
    "sales_pdf = sales_df.toPandas()\n",
    "\n",
    "# Análise de vendas por produto\n",
    "sales_summary = sales_pdf.groupby(\"Product Name\").agg({\n",
    "    \"Quote ID\": \"count\",\n",
    "    \"Product Classification\": \"first\"\n",
    "}).reset_index()\n",
    "\n",
    "# Exibir resumo\n",
    "print(sales_summary)\n",
    "\n",
    "# Salvar resumo em formato CSV\n",
    "sales_summary.to_csv(\"/dbfs/tmp/sales_summary.csv\", index=False)\n",
    "\n",
    "# Parar sessão Spark\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
